# Демо-проєкт для курсу з Data Engineering від robot_dreams

## Яку задачу вирішуємо?
Ви працюєте в українській аграрній компанії. Ваш відділ займається статистичними дослідженнями й аграрною аналітикою. Ваш керівник просить Вас організувати **періодичну** (щогодинну) статистичну аналітику з погодних умов і якості повітря в головних українських містах (Київ, Донецьк, Севастополь тощо).

Наразі, вам треба зробити **робочий прототип** з одним джерелом даних. Котрі, втім, одразу будуть використані в роботі іншими спеціалістами.

Також, з досвіду попередньої роботи, ви знаєте, що, можливо, доведеться вносити зміни в існуючий алгоритм обчислення та перераховувати дані за попередні періоди.

Вихідні дані мають бути в форматі CSV.

## Як ми її вирішуємо?
Детально всі трейдофи описані в презентації для демо-лекції. Коротко:

* Через напів-структуровану природу даних (JSON-відповідь від API), ми використовуємо Python і file-based approach (замість SQL і ELT в якійсь з СКБД).
* Так як нам треба часто запускати збір даних, ми використовуємо Airflow як **оркестратор**.
* Так як наразі обсяг даних невеликий, ми можемо використати **не-розподілений** підхід для роботи з ними. Polars -- ідеальний вибір!
* Так як нам може знадобитися робота з попередніми результатами обчислень чи сирих даних, ми використовуємо Medallion Architecture.

## Чи можна цей пайплайн покращити?
Залежить від подальших бізнес-вимог! Це гарний прототип; але, якщо йти за межі POC, є декілька очевидних покращень:

* Unit Tests
* Data Quality (гарним вибором буде **Great Expectations**)
* Виконання задач деінде, але не на Airflow executors.
* Більш зрілий Airflow deployment
* Зберігання файлів в хмарі

## Як це запустити?

* Клонуєте репозиторій на свою машину 
* Створюєте віртуальне оточення своїм улюбленим інструментом (я раджу `uv`)
* Встановлюєте усі залежності з `requirements.txt` (наприклад, командою `uv pip install -r requirements.txt`)
* Запускаєте скрипт `run_airflow_locally.sh`
* Запускаєте відповідний DAG з веб-інтерфейсу Airflow.
